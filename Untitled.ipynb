{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL hotDINA_196e237960a6d1efb93c19b6d278c568 NOW.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling stan model..\n",
      "Stan model took 127.66409945487976 s to compile\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pystan\n",
    "import time\n",
    "import pickle\n",
    "# import argparse\n",
    "\n",
    "chains = 4\n",
    "J = 10\n",
    "K = 22\n",
    "warmup = 100\n",
    "iters = 100 + warmup\n",
    "total_start = time.time()\n",
    "village_num = \"114\"\n",
    "observations = \"1000\"\n",
    "\n",
    "Y_filename      = \"Y/Y_\" + village_num + \"_\" + observations + \".npy\"\n",
    "T_filename      = \"T/T_\" + village_num + \"_\" + observations + \".npy\"\n",
    "items_filename  = \"items/items_\" + village_num + \"_all.npy\"\n",
    "\n",
    "with open(T_filename, 'rb') as f:\n",
    "    T = np.load(f)\n",
    "\n",
    "q = pd.read_csv('qmatrix.txt', header=None).to_numpy()[:J]\n",
    "I = T.shape[0]\n",
    "T = np.array(T.tolist())\n",
    "max_T = max(T)\n",
    "items_2d = -1 * np.ones((I, max(T))).astype(int)\n",
    "\n",
    "with open(items_filename, 'rb') as f:\n",
    "    items = np.load(f)\n",
    "    idx = 0\n",
    "    for i in range(I):\n",
    "        for t in range(T[i]):\n",
    "            items_2d[i][t] = items[idx]\n",
    "            if items[idx] >= J:\n",
    "                items_2d[i][t] = J-1\n",
    "            idx += 1\n",
    "\n",
    "with open(Y_filename, 'rb') as f:\n",
    "    y = np.load(f).astype(int)\n",
    "    obsY = -1 * np.ones((I, max_T)).astype(int)\n",
    "\n",
    "    for i in range(I):\n",
    "        for t in range(max_T):\n",
    "            obsY[i][t] = y[i][t][0]\n",
    "            \n",
    "    y = -1 * np.ones((I, max_T, J)).astype(int)\n",
    "    for i in range(I):\n",
    "        for t in range(T[i]):\n",
    "            j = items_2d[i][t]\n",
    "            y[i][t][j] = obsY[i][t]\n",
    "\n",
    "\n",
    "num_observations = sum(T)\n",
    "stan_model = \"\"\"\n",
    "data {\n",
    "    int I;                          // Num. students\n",
    "    int J;                          // Num. items\n",
    "    int K;                          // Num. skills\n",
    "    int max_T;                      // largest number in T\n",
    "    int T[I];                       // #opportunities for each of the I students\n",
    "    int Q[J, K];\n",
    "    int items[I, max_T];\n",
    "    int y[I,max_T,J];       // output\n",
    "}\n",
    "parameters {\n",
    "    vector[I] theta;\n",
    "    vector<lower = 0, upper = 2.5>[K] lambda0;\n",
    "    vector[K] lambda1;\n",
    "    vector<lower = 0, upper = 1>[K] learn;\n",
    "    vector<lower = 0, upper = 0.5>[J] g;\n",
    "    vector<lower = 0.5, upper = 1>[J] ss;\n",
    "}\n",
    "model {\n",
    "    \n",
    "    real lp[I, max_T, J];\n",
    "    real bern_G[I,max_T,J];\n",
    "    real bern_S[I,max_T,J];\n",
    "    real value[I,K];\n",
    "    real V[I];\n",
    "    real L[J, max_T];\n",
    "    int j;\n",
    "    \n",
    "    for (i in 1:I) {\n",
    "        V[i] = 1.0;\n",
    "    }\n",
    "    \n",
    "    for (i in 1:I) {\n",
    "        for (t in 1:T[i]) {\n",
    "            j = items[i,t];\n",
    "            if (j >= 0 && j < J && y[i,t,j] != -1) {\n",
    "                L[j,t] = 1.0;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for (i in 1:I) {\n",
    "        for (t in 1:T[i]) {\n",
    "            j = items[i,t];\n",
    "            if (j >= 0 && j < J && y[i,t,j] != -1) {\n",
    "                for (k in 1:K) {\n",
    "                    L[j,t] = L[j,t] * pow(pow(1 - learn[k], t), Q[j,k]);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    theta ~ normal(0, 1);\n",
    "    lambda0 ~ uniform(0.0, 2.5);\n",
    "    lambda1 ~ normal(0, 1);\n",
    "    learn ~ beta(1, 1);\n",
    "    ss ~ uniform(0.5, 1.0);\n",
    "    g ~ uniform(0.0, 0.5);\n",
    " \n",
    "    for (i in 1:I){\n",
    "        for (k in 1:K){\n",
    "            value[i,k] = inv_logit(1.7 * lambda1[k] * (theta[i] - lambda0[k]) );\n",
    "        }\n",
    "        j = items[i,1];\n",
    "        if (j >= 0 && j < J && y[i,1,j] != -1) {\n",
    "            for (k in 1:K) {\n",
    "                 V[i] = V[i] * pow(value[i,k], Q[j,k]);\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    }\n",
    "    \n",
    "    for (i in 1:I) {\n",
    "        for (t in 1:T[i]) {\n",
    "            j = items[i,t];\n",
    "            if (j >= 0 && j < J && y[i,t,j] != -1) {\n",
    "                bern_G[i,t,j] = pow(g[j], y[i, t, j]) * pow(1-g[j], 1-y[i,t,j]);\n",
    "                bern_S[i,t,j] = pow(ss[j], y[i, t, j]) * pow(1-ss[j], 1-y[i,t,j]);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "     \n",
    "    for (i in 1:I) {\n",
    "        j = items[i,1];\n",
    "        if (j >= 0 && j < J && y[i,1,j] != -1) {\n",
    "            lp[i,1,j] = bern_G[i,1,j] + (V[i] * (bern_S[i,1,j] - bern_G[i,1,j]));\n",
    "            target += log(lp[i,1,j]);\n",
    "        }\n",
    "    }\n",
    "     \n",
    "    for (i in 1:I) {\n",
    "        for (t in 2:T[i]) {\n",
    "            j = items[i,t];\n",
    "            if (j >= 0 && j < J && y[i,t,j] != -1) {\n",
    "                lp[i,t,j] = (L[j,t] * (bern_G[i, t, j] - bern_S[i,t,j]) * (1-V[i])) + bern_S[i,t,j];\n",
    "                target += lp[i,t,j];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "generated quantities {}\n",
    "\"\"\"\n",
    "print(\"compiling stan model..\")\n",
    "start = time.time()\n",
    "hotDINA = pystan.model.StanModel(model_code=stan_model, model_name=\"hotDINA\")\n",
    "compile_time = time.time() - start\n",
    "print(\"Stan model took\", compile_time, \"s to compile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  0,  0,  0,  0,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obsY[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting took 8.076072692871094 s for 1000 observations ( 22 SKILLS)\n",
      "Total time to compile and sample: 135.74017214775085 s\n",
      "Samples: 200 , Tune/warmup: 100 , Chains: 4\n",
      "K = 22 , #students= 2 , Observations:  1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jith/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: UserWarning: Pickling fit objects is an experimental feature!\n",
      "The relevant StanModel instance must be pickled along with this fit object.\n",
      "When unpickling the StanModel must be unpickled first.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyStan fitted and model saved as pickles/full_model_fit_114_1000.pkl\n",
      "HotDINA PyStan took  245.3924069404602 s\n",
      "Inference for Stan model: hotDINA_196e237960a6d1efb93c19b6d278c568.\n",
      "4 chains, each with iter=200; warmup=100; thin=1; \n",
      "post-warmup draws per chain=100, total post-warmup draws=400.\n",
      "\n",
      "               mean se_mean     sd   2.5%    25%     50%    75%  97.5%  n_eff   Rhat\n",
      "theta[1]       0.03    0.04   0.91  -1.68  -0.64    0.06   0.66   1.81    502   0.99\n",
      "theta[2]      -0.08    0.05   0.98  -2.05  -0.71   -0.01   0.61   1.88    465    1.0\n",
      "lambda0[1]     1.27    0.03   0.71   0.04    0.7    1.25   1.83   2.46    437    1.0\n",
      "lambda0[2]     1.23    0.03   0.74   0.06   0.59    1.19   1.88   2.45    810   0.99\n",
      "lambda0[3]     1.29    0.03   0.73   0.04   0.71    1.27   1.91   2.48    643   0.99\n",
      "lambda0[4]     1.26    0.03   0.74   0.06   0.58     1.3   1.93   2.44    847   0.99\n",
      "lambda0[5]     1.26    0.02   0.71   0.07   0.69    1.28   1.82   2.44   1078   0.99\n",
      "lambda0[6]     1.24    0.03   0.77   0.03   0.54    1.24   1.88   2.47    659   0.99\n",
      "lambda0[7]     1.23    0.03   0.75   0.07   0.55    1.18   1.95   2.43    620    1.0\n",
      "lambda0[8]     1.26    0.03   0.73   0.05   0.63    1.27    1.9   2.47    443    1.0\n",
      "lambda0[9]     1.25    0.02   0.72   0.08   0.62    1.26   1.87   2.42   1016    1.0\n",
      "lambda0[10]    1.23    0.03   0.74   0.09   0.54    1.22   1.89   2.42    807    1.0\n",
      "lambda0[11]    1.21    0.03   0.73   0.07   0.55    1.18   1.84   2.44    543    1.0\n",
      "lambda0[12]    1.25    0.03   0.74   0.06   0.58    1.26   1.93   2.42    849    1.0\n",
      "lambda0[13]    1.24    0.03    0.7   0.09   0.64    1.26   1.81   2.43    609    1.0\n",
      "lambda0[14]    1.25    0.03   0.74   0.08   0.56    1.26   1.89   2.43    647    1.0\n",
      "lambda0[15]    1.24    0.02   0.71   0.08   0.61    1.21   1.83   2.44    837    1.0\n",
      "lambda0[16]    1.26    0.02   0.74   0.08   0.58    1.27   1.94   2.42    924   0.99\n",
      "lambda0[17]    1.24    0.02   0.67   0.09   0.68    1.25   1.75   2.42   1042   0.99\n",
      "lambda0[18]    1.23    0.03   0.68   0.09   0.65     1.2   1.83   2.38    748   0.99\n",
      "lambda0[19]    1.28    0.03   0.74   0.05   0.62    1.32   1.94   2.46    709   0.99\n",
      "lambda0[20]     1.3    0.02   0.75   0.05   0.68    1.28    2.0   2.45    967    1.0\n",
      "lambda0[21]    1.24    0.03   0.72   0.08    0.6    1.23   1.85   2.42    752    1.0\n",
      "lambda0[22]    1.24    0.02   0.75   0.05   0.51    1.23   1.96   2.44   1036   0.99\n",
      "lambda1[1]     0.02    0.05   0.97  -1.91  -0.59 -5.7e-3   0.69   2.03    350   1.01\n",
      "lambda1[2]    -0.05    0.06   1.12  -2.18  -0.84   -0.05   0.63   2.16    393    1.0\n",
      "lambda1[3]     0.05    0.05   1.07  -2.18  -0.57    0.05   0.72   2.13    439    1.0\n",
      "lambda1[4]     0.08    0.04   0.92  -1.73  -0.54    0.11   0.76   1.85    446    1.0\n",
      "lambda1[5]    -0.07    0.05   1.04  -2.14   -0.8   -0.05   0.65    2.1    383    1.0\n",
      "lambda1[6]    -0.04    0.04   0.94  -2.09  -0.63   -0.04   0.54   1.85    446    1.0\n",
      "lambda1[7]    -0.02    0.04   0.97  -1.82   -0.7   -0.01   0.71   1.94    509    1.0\n",
      "lambda1[8]     0.02    0.05   1.03  -1.86  -0.69   -0.02   0.72   2.07    373    1.0\n",
      "lambda1[9]    -0.02    0.04   0.92  -1.92  -0.61   -0.03   0.56   1.89    461    1.0\n",
      "lambda1[10]  9.2e-3    0.06   0.98   -1.9  -0.64  5.2e-3   0.73   1.82    299   1.01\n",
      "lambda1[11]    0.04    0.05   0.97  -2.06  -0.59    0.04    0.7   1.87    385   0.99\n",
      "lambda1[12]    0.02    0.05   1.02  -1.95  -0.64    0.02   0.66   1.92    472   0.99\n",
      "lambda1[13]   -0.05    0.05   1.05  -2.21  -0.72   -0.06   0.61   1.92    464   1.01\n",
      "lambda1[14]    0.04    0.05   1.02  -2.04  -0.61    0.04   0.78   1.89    446    1.0\n",
      "lambda1[15]   -0.03    0.05   1.01  -1.96  -0.74   -0.06   0.69   1.88    464    1.0\n",
      "lambda1[16]    0.02    0.04   0.97  -2.08  -0.56    0.05   0.62   2.06    486    1.0\n",
      "lambda1[17] -6.9e-4    0.05   1.04  -2.04  -0.68    0.05   0.69   1.87    386    1.0\n",
      "lambda1[18]  6.3e-3    0.05   1.08  -2.16  -0.72    0.06   0.67   2.25    520   0.99\n",
      "lambda1[19]    0.04    0.05   1.09  -2.34  -0.62    0.08   0.77   1.97    449    1.0\n",
      "lambda1[20]    0.06    0.05   0.96  -1.99   -0.6    0.16   0.69   1.86    416   1.01\n",
      "lambda1[21]    0.01    0.05   1.07  -2.11  -0.75    0.12   0.76   1.96    494    1.0\n",
      "lambda1[22]   -0.11    0.05   0.96  -1.89  -0.81   -0.11   0.61   1.63    395   0.99\n",
      "learn[1]       0.51    0.01   0.27   0.03    0.3     0.5   0.74   0.97    456   1.01\n",
      "learn[2]       0.49    0.01   0.26   0.05   0.28    0.49    0.7   0.93    602    1.0\n",
      "learn[3]        0.5    0.01   0.29   0.02   0.28    0.51   0.75   0.97    795   0.99\n",
      "learn[4]        0.5    0.01   0.29   0.02   0.24    0.51   0.73   0.98    621    1.0\n",
      "learn[5]       0.52    0.01    0.3   0.03   0.26    0.54   0.77   0.99    745   0.99\n",
      "learn[6]       0.49    0.01   0.31   0.02   0.22    0.49   0.77   0.98    838   0.99\n",
      "learn[7]        0.5    0.01    0.3   0.03   0.22     0.5   0.78   0.98    674   0.99\n",
      "learn[8]       0.51    0.01   0.29   0.04   0.25    0.52   0.77   0.96    831    1.0\n",
      "learn[9]       0.51  9.2e-3   0.29   0.01   0.26    0.52   0.76   0.99   1005    1.0\n",
      "learn[10]      0.47  9.6e-3   0.27   0.02   0.25    0.47   0.67   0.96    787    1.0\n",
      "learn[11]       0.5    0.01    0.3   0.01   0.24    0.48   0.77   0.98    451   0.99\n",
      "learn[12]       0.5  9.6e-3    0.3   0.02   0.25    0.51   0.75   0.99    972   0.99\n",
      "learn[13]       0.5    0.01   0.29   0.02   0.26    0.49   0.76   0.99    684    1.0\n",
      "learn[14]      0.49    0.01   0.27   0.03   0.26    0.48   0.72   0.96    683   0.99\n",
      "learn[15]       0.5    0.01    0.3   0.02   0.23     0.5   0.77   0.98    645    1.0\n",
      "learn[16]       0.5  9.5e-3   0.28   0.03   0.27    0.48   0.73   0.97    857    1.0\n",
      "learn[17]      0.51  9.5e-3   0.28   0.04   0.26    0.49   0.75   0.97    857    1.0\n",
      "learn[18]       0.5  9.7e-3    0.3   0.02   0.24    0.49   0.78   0.98    937   0.99\n",
      "learn[19]       0.5 10.0e-3   0.29   0.02   0.23     0.5   0.78   0.97    873   0.99\n",
      "learn[20]      0.48  9.8e-3   0.27   0.03   0.25    0.48   0.71   0.96    789    1.0\n",
      "learn[21]       0.5  9.9e-3   0.29   0.02   0.26    0.48   0.75   0.98    848   0.99\n",
      "learn[22]      0.51    0.01   0.31   0.01   0.24     0.5   0.79   0.98    772   0.99\n",
      "g[1]           0.25  7.1e-3   0.16 3.3e-3   0.11    0.25   0.39    0.5    492   0.99\n",
      "g[2]           0.25  6.8e-3   0.16 9.9e-3    0.1    0.26   0.39   0.49    513   0.99\n",
      "g[3]           0.25  4.7e-3   0.14   0.01   0.12    0.25   0.38   0.49    935   0.99\n",
      "g[4]           0.25  5.9e-3   0.15 5.8e-3   0.13    0.24   0.37   0.49    657   0.99\n",
      "g[5]           0.25  5.4e-3   0.15   0.01   0.13    0.24   0.38   0.49    755   0.99\n",
      "g[6]           0.27  4.9e-3   0.15   0.02   0.14    0.27    0.4   0.49    905   0.99\n",
      "g[7]           0.25  4.9e-3   0.14   0.01   0.14    0.24   0.37   0.48    775   0.99\n",
      "g[8]           0.25  5.4e-3   0.14 8.7e-3   0.12    0.25   0.37   0.48    688    1.0\n",
      "g[9]           0.25  5.9e-3   0.14   0.01   0.12    0.24   0.36   0.49    590    1.0\n",
      "g[10]          0.24  5.4e-3   0.14   0.01   0.12    0.24   0.36   0.49    713    1.0\n",
      "ss[1]          0.75  5.2e-3   0.14   0.52   0.63    0.76   0.87   0.98    691    1.0\n",
      "ss[2]          0.74  4.8e-3   0.15   0.52    0.6    0.74   0.88   0.98    918    1.0\n",
      "ss[3]          0.75  5.3e-3   0.15   0.51   0.61    0.75   0.88   0.99    810   0.99\n",
      "ss[4]          0.75  5.1e-3   0.14   0.52   0.64    0.74   0.86   0.99    743    1.0\n",
      "ss[5]          0.75  5.6e-3   0.15   0.51   0.61    0.75   0.87   0.99    708    1.0\n",
      "ss[6]          0.76  5.4e-3   0.15   0.51   0.63    0.77   0.89   0.99    792    1.0\n",
      "ss[7]          0.75  5.2e-3   0.14   0.52   0.63    0.75   0.87   0.98    714    1.0\n",
      "ss[8]          0.76  5.5e-3   0.15   0.51   0.62    0.77   0.89   0.99    761   0.99\n",
      "ss[9]          0.76  4.1e-3   0.15   0.51   0.63    0.77   0.89   0.99   1299   0.99\n",
      "ss[10]         0.75  4.9e-3   0.16    0.5   0.61    0.76    0.9    1.0   1032   0.99\n",
      "lp__         -135.0    0.69   7.84 -151.2 -140.3  -135.4 -129.4 -120.5    129   1.03\n",
      "\n",
      "Samples were drawn using NUTS at Wed Aug  5 18:13:19 2020.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n"
     ]
    }
   ],
   "source": [
    "fitting_start_time = time.time()\n",
    "hotDINA_fit = hotDINA.sampling(data={'I': I,\n",
    "                                     'J': J,\n",
    "                                     'K': K,\n",
    "                                     'max_T': max_T,\n",
    "                                     'T': T,\n",
    "                                     'Q': q,\n",
    "                                     'items': items_2d,\n",
    "                                     'y': y\n",
    "                                     },\n",
    "                               iter=iters,\n",
    "                               chains=chains, \n",
    "                               warmup=warmup)\n",
    "\n",
    "fitting_end_time = time.time()\n",
    "fitting_time = fitting_end_time - fitting_start_time\n",
    "print(\"Fitting took\", fitting_time, \"s for\", sum(T), \"observations (\" , K , \"SKILLS)\")\n",
    "total_time = compile_time + fitting_time\n",
    "print(\"Total time to compile and sample:\", total_time, \"s\")\n",
    "print(\"Samples:\", iters, \", Tune/warmup:\", warmup, \", Chains:\", chains)\n",
    "print(\"K =\", K, \", #students=\", I, \", Observations: \", sum(T))\n",
    "\n",
    "pickle_file = \"pickles/full_model_fit_\" + village_num + \"_\" + observations + \".pkl\"\n",
    "\n",
    "with open(pickle_file, \"wb\") as f:\n",
    "    pickle.dump({'stan_model' : stan_model, \n",
    "                 'pystan_model' : hotDINA,\n",
    "                 'fit' : hotDINA_fit}, f, protocol=-1)\n",
    "print(\"PyStan fitted and model saved as \" + pickle_file)\n",
    "total_end = time.time()\n",
    "print(\"HotDINA PyStan took \", total_end - total_start, \"s\")\n",
    "print(hotDINA_fit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
