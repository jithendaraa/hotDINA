+ python fit_hotDINA_skill_pystan.py -v 130 -o all -w 5000 -i 15000
INFO:pystan:COMPILING THE C++ CODE FOR MODEL hotDINA_skill_0169052b413d03a229c34988eca0dc2d NOW.
compiling stan model..
/home/jith/anaconda3/lib/python3.8/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /local/10666874/tmp/tmp83rq4c5k/stanfit4hotDINA_skill_0169052b413d03a229c34988eca0dc2d_5937819118109568582.pyx
  tree = Parsing.p_module(s, pxd, full_module_name)
Stan model took 56.180264711380005 s to compile

Gradient evaluation took 0.4 seconds
1000 transitions using 10 leapfrog steps per transition would take 4000 seconds.
Adjust your expectations accordingly!



Gradient evaluation took 0.47 seconds
1000 transitions using 10 leapfrog steps per transition would take 4700 seconds.
Adjust your expectations accordingly!



Gradient evaluation took 0.42 seconds
1000 transitions using 10 leapfrog steps per transition would take 4200 seconds.
Adjust your expectations accordingly!



Gradient evaluation took 0.45 seconds
1000 transitions using 10 leapfrog steps per transition would take 4500 seconds.
Adjust your expectations accordingly!


Iteration:     1 / 15000 [  0%]  (Warmup)
Iteration:     1 / 15000 [  0%]  (Warmup)
Iteration:     1 / 15000 [  0%]  (Warmup)
Iteration:     1 / 15000 [  0%]  (Warmup)
Iteration:  1500 / 15000 [ 10%]  (Warmup)
Iteration:  1500 / 15000 [ 10%]  (Warmup)
Iteration:  1500 / 15000 [ 10%]  (Warmup)
Iteration:  1500 / 15000 [ 10%]  (Warmup)
Iteration:  3000 / 15000 [ 20%]  (Warmup)
Iteration:  3000 / 15000 [ 20%]  (Warmup)
Iteration:  3000 / 15000 [ 20%]  (Warmup)
Iteration:  3000 / 15000 [ 20%]  (Warmup)
Iteration:  4500 / 15000 [ 30%]  (Warmup)
Iteration:  4500 / 15000 [ 30%]  (Warmup)
Iteration:  5001 / 15000 [ 33%]  (Sampling)
Iteration:  4500 / 15000 [ 30%]  (Warmup)
Iteration:  5001 / 15000 [ 33%]  (Sampling)
Iteration:  5001 / 15000 [ 33%]  (Sampling)
Iteration:  4500 / 15000 [ 30%]  (Warmup)
Iteration:  5001 / 15000 [ 33%]  (Sampling)
Iteration:  6500 / 15000 [ 43%]  (Sampling)
Iteration:  6500 / 15000 [ 43%]  (Sampling)
Iteration:  6500 / 15000 [ 43%]  (Sampling)
Iteration:  6500 / 15000 [ 43%]  (Sampling)
Iteration:  8000 / 15000 [ 53%]  (Sampling)
Iteration:  8000 / 15000 [ 53%]  (Sampling)
Iteration:  8000 / 15000 [ 53%]  (Sampling)
Iteration:  8000 / 15000 [ 53%]  (Sampling)
Iteration:  9500 / 15000 [ 63%]  (Sampling)
Iteration:  9500 / 15000 [ 63%]  (Sampling)
Iteration:  9500 / 15000 [ 63%]  (Sampling)
Iteration:  9500 / 15000 [ 63%]  (Sampling)
Iteration: 11000 / 15000 [ 73%]  (Sampling)
Iteration: 11000 / 15000 [ 73%]  (Sampling)
Iteration: 11000 / 15000 [ 73%]  (Sampling)
Iteration: 11000 / 15000 [ 73%]  (Sampling)
Iteration: 12500 / 15000 [ 83%]  (Sampling)
Iteration: 12500 / 15000 [ 83%]  (Sampling)
Iteration: 12500 / 15000 [ 83%]  (Sampling)
Iteration: 12500 / 15000 [ 83%]  (Sampling)
Iteration: 14000 / 15000 [ 93%]  (Sampling)
Iteration: 14000 / 15000 [ 93%]  (Sampling)
Iteration: 14000 / 15000 [ 93%]  (Sampling)
Iteration: 15000 / 15000 [100%]  (Sampling)

 Elapsed Time: 58828.8 seconds (Warm-up)
               162718 seconds (Sampling)
               221547 seconds (Total)

Iteration: 14000 / 15000 [ 93%]  (Sampling)
Iteration: 15000 / 15000 [100%]  (Sampling)

 Elapsed Time: 66501.6 seconds (Warm-up)
               162134 seconds (Sampling)
               228635 seconds (Total)

Iteration: 15000 / 15000 [100%]  (Sampling)

 Elapsed Time: 62448.6 seconds (Warm-up)
               174152 seconds (Sampling)
               236601 seconds (Total)

Iteration: 15000 / 15000 [100%]  (Sampling)

 Elapsed Time: 81095.8 seconds (Warm-up)
               160656 seconds (Sampling)
               241752 seconds (Total)

WARNING:pystan:73 of 40000 iterations ended with a divergence (0.1825%).
WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.
fit_hotDINA_skill_pystan.py:137: UserWarning: Pickling fit objects is an experimental feature!
The relevant StanModel instance must be pickled along with this fit object.
When unpickling the StanModel must be unpickled first.
  pickle.dump({'stan_model' : stan_model,
Fitting took 241765.25742149353 s for 42010 observations ( 22 SKILLS)
Total time to compile and sample: 241821.4376862049 s
Samples: 15000 , Tune/warmup: 5000 , Chains: 4
K = 22 , #students= 8 , Observations:  42010
PyStan fitted and model saved as pickles/skill_fit_model/skill_model_fit_130_all.pickle
HotDINA PyStan took  241821.89691591263 s
Inference for Stan model: hotDINA_skill_0169052b413d03a229c34988eca0dc2d.
4 chains, each with iter=15000; warmup=5000; thin=1; 
post-warmup draws per chain=10000, total post-warmup draws=40000.

              mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat
theta[1]     -0.17  4.2e-3   0.93  -2.03  -0.79  -0.16   0.45   1.63  49411    1.0
theta[2]      1.77  4.3e-3   0.64   0.61   1.32   1.74   2.18   3.08  21599    1.0
theta[3]     -0.65 10.0e-3   0.72  -2.23   -1.1  -0.57  -0.14   0.59   5242    1.0
theta[4]     -0.73    0.02   1.13  -2.68  -1.55  -0.84 1.6e-3   1.73   3589    1.0
theta[5]     -0.38  5.1e-3   0.78  -2.02  -0.83  -0.35   0.09   1.16  23429    1.0
theta[6]      0.28  5.3e-3   0.55  -0.88  -0.05   0.31   0.65    1.3  11019    1.0
theta[7]     -0.48  9.5e-3    0.7  -2.05   -0.9  -0.41 6.3e-3   0.72   5443    1.0
theta[8]      0.14  3.5e-3   0.47  -0.75  -0.17   0.14   0.46   1.07  17683    1.0
lambda0[1]    1.25  2.9e-3   0.73   0.06   0.63   1.26   1.89   2.44  63052    1.0
lambda0[2]    1.25  2.8e-3   0.72   0.06   0.63   1.25   1.87   2.44  63999    1.0
lambda0[3]    1.25  3.0e-3   0.72   0.06   0.62   1.24   1.87   2.44  59262    1.0
lambda0[4]    1.25  3.0e-3   0.72   0.07   0.63   1.26   1.88   2.44  57940    1.0
lambda0[5]    1.42  3.6e-3   0.71   0.09   0.84   1.51   2.03   2.46  40089    1.0
lambda0[6]    1.49  3.4e-3   0.68   0.12   0.98    1.6   2.08   2.46  41417    1.0
lambda0[7]    1.25  2.9e-3   0.72   0.06   0.62   1.25   1.88   2.44  60225    1.0
lambda0[8]    1.25  2.8e-3   0.72   0.06   0.62   1.26   1.88   2.44  67006    1.0
lambda0[9]    1.27  3.0e-3   0.72   0.06   0.65   1.29    1.9   2.44  58037    1.0
lambda0[10]   1.25  2.9e-3   0.72   0.06   0.63   1.24   1.86   2.44  62382    1.0
lambda0[11]    1.1  3.4e-3   0.72   0.04   0.46   1.02   1.69   2.41  45000    1.0
lambda0[12]   1.19  3.6e-3   0.74   0.05   0.53   1.16   1.84   2.43  40951    1.0
lambda0[13]   1.09  4.0e-3   0.53   0.15    0.7   1.07   1.45   2.19  17754    1.0
lambda0[14]   1.25  2.8e-3   0.72   0.06   0.63   1.26   1.88   2.44  64675    1.0
lambda0[15]   1.25  2.8e-3   0.72   0.07   0.63   1.25   1.88   2.44  66868    1.0
lambda0[16]   1.25  2.9e-3   0.73   0.06   0.62   1.25   1.88   2.44  61731    1.0
lambda0[17]   1.25  2.7e-3   0.72   0.06   0.63   1.25   1.87   2.44  70132    1.0
lambda0[18]   1.25  2.8e-3   0.72   0.06   0.63   1.26   1.87   2.44  67109    1.0
lambda0[19]   1.25  3.0e-3   0.72   0.06   0.61   1.25   1.88   2.44  60269    1.0
lambda0[20]   1.25  2.8e-3   0.72   0.06   0.62   1.26   1.87   2.44  68403    1.0
lambda0[21]   1.24  2.9e-3   0.73   0.06   0.62   1.24   1.87   2.44  64378    1.0
lambda0[22]   1.25  2.9e-3   0.72   0.07   0.62   1.25   1.88   2.44  61192    1.0
lambda1[1]  1.3e-5  4.0e-3   1.01  -1.97  -0.68-6.3e-3   0.69   1.97  62743    1.0
lambda1[2]  2.3e-3  4.1e-3    1.0  -1.93  -0.67-3.1e-3   0.67   1.94  59140    1.0
lambda1[3] -6.5e-3  3.9e-3   1.01  -1.98  -0.69-4.7e-3   0.68   1.96  65220    1.0
lambda1[4]   -0.03  4.1e-3   1.01   -2.0  -0.72  -0.03   0.65   1.95  61816    1.0
lambda1[5]   -0.69  5.0e-3   0.89  -2.33  -1.26  -0.74  -0.22   1.34  31551    1.0
lambda1[6]   -0.87  4.6e-3   0.81  -2.42  -1.38  -0.88  -0.42   0.99  31138    1.0
lambda1[7]  2.5e-3  3.9e-3   0.98  -1.93  -0.65 2.0e-3   0.67   1.93  62811    1.0
lambda1[8] -1.7e-3  3.9e-3   0.99  -1.96  -0.68 1.2e-3   0.66   1.95  63624    1.0
lambda1[9]   -0.47  4.3e-3   0.93  -2.21  -1.07  -0.51   0.07   1.56  45793    1.0
lambda1[10]   0.12  4.2e-3   0.99  -1.89  -0.53   0.15    0.8   2.04  57169    1.0
lambda1[11]   0.19  6.8e-3   0.81  -1.43  -0.29   0.14   0.67   1.91  14029    1.0
lambda1[12]  -0.14  7.6e-3   0.87  -1.89  -0.66  -0.16   0.36   1.71  13297    1.0
lambda1[13]  -1.28  3.3e-3   0.46  -2.37  -1.55  -1.21  -0.94  -0.59  19764    1.0
lambda1[14]  -0.03  4.0e-3   0.99  -1.96   -0.7  -0.03   0.65   1.93  63091    1.0
lambda1[15] 2.5e-3  4.0e-3   1.01  -2.02  -0.67 4.0e-3   0.68   1.99  63439    1.0
lambda1[16]   0.01  4.0e-3    1.0  -1.94  -0.66 9.0e-3   0.69   1.99  63058    1.0
lambda1[17] 5.1e-3  4.2e-3    1.0  -1.95  -0.67 5.6e-3   0.68   1.96  56885    1.0
lambda1[18] 3.5e-3  4.2e-3    1.0  -1.97  -0.67 4.5e-4   0.68   1.95  57366    1.0
lambda1[19] 8.0e-4  4.1e-3   0.99  -1.96  -0.67 5.3e-3   0.67   1.95  59682    1.0
lambda1[20]-3.6e-3  3.9e-3   0.98  -1.93  -0.67-1.9e-3   0.66   1.94  61880    1.0
lambda1[21]-8.1e-3  4.0e-3    1.0  -1.96  -0.68  -0.01   0.66   1.96  61963    1.0
lambda1[22]-1.7e-3  4.0e-3   1.01  -1.98  -0.68-5.6e-3   0.68   1.98  62978    1.0
learn[1]      0.51  1.2e-3   0.29   0.04   0.26   0.51   0.76   0.98  57856    1.0
learn[2]       0.5  1.1e-3   0.29   0.03   0.25    0.5   0.75   0.97  64134    1.0
learn[3]       0.5  1.1e-3   0.29   0.02   0.25    0.5   0.75   0.97  64964    1.0
learn[4]      0.55  1.1e-3   0.26    0.1   0.33   0.55   0.78   0.98  55211    1.0
learn[5]      0.61  1.1e-3   0.25   0.09   0.42   0.63   0.82   0.98  47509    1.0
learn[6]      0.61  1.1e-3   0.25    0.1   0.43   0.64   0.82   0.98  49134    1.0
learn[7]       0.5  1.2e-3   0.29   0.02   0.25    0.5   0.75   0.98  61988    1.0
learn[8]       0.5  1.2e-3   0.29   0.03   0.25    0.5   0.75   0.97  59226    1.0
learn[9]      0.58  1.1e-3   0.26   0.09   0.36    0.6    0.8   0.98  56764    1.0
learn[10]     0.54  1.1e-3   0.28   0.04    0.3   0.55   0.78   0.98  62290    1.0
learn[11]     0.71  8.7e-4    0.2   0.29   0.58   0.74   0.87   0.99  50402    1.0
learn[12]     0.71  9.0e-4    0.2   0.26   0.57   0.74   0.88   0.99  50798    1.0
learn[13]   3.9e-4  8.0e-6 4.3e-4 7.6e-7 1.2e-5 3.1e-4 6.6e-4 1.3e-3   2848    1.0
learn[14]     0.53  1.1e-3   0.27   0.07   0.29   0.53   0.76   0.98  56767    1.0
learn[15]      0.5  1.2e-3   0.29   0.03   0.25    0.5   0.75   0.97  56118    1.0
learn[16]      0.5  1.1e-3   0.29   0.02   0.25    0.5   0.75   0.97  63874    1.0
learn[17]      0.5  1.2e-3   0.29   0.02   0.25    0.5   0.75   0.97  61196    1.0
learn[18]      0.5  1.2e-3   0.29   0.02   0.25    0.5   0.75   0.98  60644    1.0
learn[19]      0.5  1.2e-3   0.29   0.02   0.25    0.5   0.75   0.98  56104    1.0
learn[20]      0.5  1.1e-3   0.29   0.02   0.25    0.5   0.75   0.98  64187    1.0
learn[21]      0.5  1.1e-3   0.29   0.03   0.25    0.5   0.75   0.97  63546    1.0
learn[22]      0.5  1.1e-3   0.29   0.03   0.25    0.5   0.75   0.97  66497    1.0
g[1]          0.25  5.6e-4   0.14   0.01   0.13   0.25   0.37   0.49  66751    1.0
g[2]          0.25  5.7e-4   0.14   0.01   0.13   0.25   0.37   0.49  63067    1.0
g[3]          0.25  5.8e-4   0.14   0.01   0.12   0.25   0.38   0.49  62614    1.0
g[4]          0.25  5.7e-4   0.14   0.01   0.13   0.26   0.38   0.49  64175    1.0
g[5]          0.28  6.0e-4   0.14   0.02   0.17    0.3   0.41   0.49  56968    1.0
g[6]          0.28  6.5e-4   0.14   0.02   0.16    0.3   0.41   0.49  48085    1.0
g[7]          0.25  5.7e-4   0.14   0.01   0.13   0.25   0.37   0.49  63537    1.0
g[8]          0.25  6.0e-4   0.14   0.01   0.13   0.25   0.38   0.49  58472    1.0
g[9]          0.27  6.0e-4   0.14   0.01   0.15   0.28    0.4   0.49  57063    1.0
g[10]         0.24  5.9e-4   0.14   0.01   0.11   0.23   0.36   0.49  59329    1.0
g[11]         0.29  6.0e-4   0.14   0.02   0.19   0.31   0.41   0.49  53040    1.0
g[12]          0.3  6.1e-4   0.14   0.03   0.19   0.32   0.42   0.49  51121    1.0
g[13]         0.19  5.7e-4   0.13 8.2e-3   0.09   0.18   0.28   0.46  49507    1.0
g[14]         0.25  5.7e-4   0.14   0.01   0.13   0.25   0.38   0.49  63415    1.0
g[15]         0.25  5.8e-4   0.14   0.01   0.13   0.25   0.38   0.49  63188    1.0
g[16]         0.25  5.8e-4   0.14   0.01   0.12   0.25   0.38   0.49  62805    1.0
g[17]         0.25  5.5e-4   0.14   0.01   0.13   0.25   0.37   0.49  67743    1.0
g[18]         0.25  5.7e-4   0.14   0.01   0.13   0.25   0.38   0.49  62904    1.0
g[19]         0.25  5.5e-4   0.14   0.01   0.12   0.25   0.38   0.49  69263    1.0
g[20]         0.25  5.6e-4   0.14   0.01   0.12   0.25   0.38   0.49  67239    1.0
g[21]         0.25  5.9e-4   0.14   0.01   0.12   0.25   0.37   0.49  59472    1.0
g[22]         0.25  5.5e-4   0.14   0.01   0.13   0.25   0.37   0.49  69130    1.0
ss[1]         0.67  3.9e-5   0.01   0.65   0.66   0.67   0.67   0.69  67970    1.0
ss[2]         0.75  4.8e-5   0.01   0.73   0.74   0.75   0.76   0.77  61639    1.0
ss[3]         0.75  6.0e-4   0.14   0.51   0.62   0.75   0.87   0.99  57747    1.0
ss[4]         0.73  1.1e-4   0.03   0.67   0.71   0.73   0.74   0.78  63686    1.0
ss[5]         0.89  1.9e-5 4.9e-3   0.88   0.88   0.89   0.89    0.9  63799    1.0
ss[6]         0.87  1.1e-5 2.7e-3   0.87   0.87   0.87   0.88   0.88  63391    1.0
ss[7]         0.85  2.5e-5 6.1e-3   0.84   0.85   0.85   0.86   0.86  58595    1.0
ss[8]         0.73  3.8e-4   0.08   0.57   0.68   0.74   0.79   0.87  41884    1.0
ss[9]         0.88  1.3e-5 3.2e-3   0.87   0.88   0.88   0.88   0.89  58054    1.0
ss[10]        0.62  1.4e-4   0.03   0.55   0.59   0.62   0.64   0.68  52415    1.0
ss[11]         1.0  1.8e-6 4.5e-4   0.99    1.0    1.0    1.0    1.0  61293    1.0
ss[12]        0.96  4.4e-6 1.1e-3   0.96   0.96   0.96   0.96   0.96  61215    1.0
ss[13]        0.95  7.2e-4   0.03   0.92   0.93   0.94   0.98    1.0   1555    1.0
ss[14]        0.82  7.0e-5   0.02   0.79   0.81   0.82   0.83   0.85  59726    1.0
ss[15]        0.62  6.8e-5   0.02   0.59   0.61   0.62   0.63   0.65  60178    1.0
ss[16]        0.99  2.7e-5 6.4e-3   0.98   0.99    1.0    1.0    1.0  56664    1.0
ss[17]         0.6  1.0e-4   0.02   0.55   0.58    0.6   0.61   0.64  54256    1.0
ss[18]        0.75  5.8e-4   0.14   0.51   0.62   0.75   0.88   0.99  62424    1.0
ss[19]        0.75  5.7e-4   0.14   0.51   0.63   0.75   0.87   0.99  63085    1.0
ss[20]        0.75  5.7e-4   0.14   0.51   0.63   0.75   0.87   0.99  63759    1.0
ss[21]        0.75  5.7e-4   0.14   0.51   0.63   0.75   0.88   0.99  65403    1.0
ss[22]        0.75  5.8e-4   0.14   0.51   0.63   0.75   0.87   0.99  62067    1.0
lp__        -2.2e4    0.08    8.8 -2.2e4 -2.2e4 -2.2e4 -2.2e4 -2.2e4  11044    1.0

Samples were drawn using NUTS at Tue Sep  1 11:07:39 2020.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
